---
title: scrapy-去重url
date: 2017-07-08 17:49:51
tags: scrapy
---

## introduce

爬虫在全站爬取的时候要整体爬去资源，必不可免要对页面内的url进行分析，然后遍历全局，之前介绍过遍历的2种方法：[深度优先&广度优先](/2017/07/08/scrapy-深度-广度优先/)

同时也说了二叉树是理想情况，正常情况肯定存在环路，所以我们要去环路。

去环路的办法：去重url

## 去重复手段

下面介绍手段是对算法和数据存储的对比，必须知道换算关系

> 1字节=1byte=8bite=8位
> 1G = 1024 * 1024 * 1024 byte
> 1M = 1024 * 1024 byte
> 1K = 1024 byte

- 保存到mysql中
    - 效率低下

- 保存到redis中的`Set`集合中，O(1)获取url
    - 内存占用高
        - 10000 0000(1亿url) * 50byte(url长度) * 2(python采用unicde编码) / 1024 / 1024 / 1024 = 9.31G

- md5(url)
    - `！！！！！！Scrapy采用的方式！！！！！！！`
    - 内存占用：[深度优先&广度优先]
        - md5(url) = 128bite = 16byte
        - 10000 0000(1亿url) * 16byte * 2 / 1024 / 1024 / 1024 = 2.9G (2~3G)

- bitmap
    - hash(url)
    - hash后命中8bit种的一位
    - 冲突高
        - 多个不同url命中同一位
        - 可以通过继续寻址来解决
    - 数据大小
        - 10000 0000(1亿url) * 1bit / 8 / 1024 / 1024 / 1024 = 12 M(真是情况稍大，但是数量级不会变)

- bloomfilter
    - 对bitemap改进，采用多重hash，降低冲突
    - `！！！！！！分布式Scrapy采用的方式！！！！！！！`
